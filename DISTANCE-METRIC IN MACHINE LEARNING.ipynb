{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-13T14:11:55.775582Z","iopub.execute_input":"2022-04-13T14:11:55.776247Z","iopub.status.idle":"2022-04-13T14:11:55.783699Z","shell.execute_reply.started":"2022-04-13T14:11:55.776198Z","shell.execute_reply":"2022-04-13T14:11:55.782811Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"Distance metrics are a key part of several machine learning algorithms. A distance measure is an objective score that summarizes the relative difference between two objects in a problem domain.\n\n\nThese distance metrics are used in both supervised and unsupervised learning, generally to calculate the similarity between data points.\n\nLet’s say we want to create clusters using the K-Means Clustering or k-Nearest Neighbour algorithm to solve a classification or regression problem. How will you define the similarity between different observations here? How can we say that two points are similar to each other?\n\nWe can calculate the distance between points and then define the similarity between them. How do we calculate this distance and what are the different distance metrics in machine learning?\n\n\nThere are several ways of measuring distance between point a and b in d dimensions with closer distance implying similarity.\n\nIn this project, I will introduce different approaches to measuring distance. These approachses are:\n\n1) Minkowski Distance Metric\n    \n    * Mahanttan Distance:\n    \n    * Euclidean Distance:\n\n2) Weighted Similarity\n\n3) Cosine Similarity\n\n4) Kullback-Leibler Divergence\n\n5) Jensen-Shannon\n\nFollow-up section is the code implementation with the case study is 37 pharmarceutical companies and theirs number of patent applied in each IPC. The goal of this section is how to apply different Similarity Measure Algorithm to calculate the Technonoly Similarity between the companies. ","metadata":{}},{"cell_type":"markdown","source":"# Kernel Layout:\n\n# <a href='#1'>  1) Thereotical and Mathematic Background: </a>\n\n## <a href='#1.1'> 1.1) Minkowski  Distance Metric: </a>\n\n## <a href='#1.2'> 1.2) Eucclidean Distance Metric: </a>\n\n## <a href='#1.3'> 1.3) Manhattan Distance Metric: </a>\n\n## <a href='#1.4'> 1.4) Cosine Distance Metric & Cosine Similarity: </a>\n\n## <a href ='#1.5'> 1.5) Jemsen-Shannon Distance Metric: </a>\n\n# <a href='#2'>  2) CODE IMPLEMENTATION: </a>\n\n## <a href='#2.1'> 2.1) Minkowski  Distance Metric: </a>\n\n## <a href='#2.2'> 2.2) Eucclidean  Distance Metric: </a>\n\n## <a href='#2.3'> 2.3) Mahhatan Distance Metric: <a>\n    \n## <a href='#2.4'> 2.4) Cosine Distance Metric: </a>\n    \n## <a href='#2.5'> 2.5) Jemsen-Shannon Distance Metric: </a>\n    \n# <a href='#3'>  3) COMPANY GROUPING: </a>   \n    \n## <a href='#3.1'> 3.1) k-mean Cluster: Elbow Method: </a>\n    \n## <a href='#3.2'> 3.2) k-mean Cluster: Silhouette Method: </a>\n\n## <a href='#3.3'> 3.3) Agglomerative Hierarchical Method: </a>\n    \n## <a href='#3.4'> 3.4) Company Grouping Summary: </a>","metadata":{}},{"cell_type":"markdown","source":"## <a id='1'> <h1><center> 1. THREOTICAL AND MATHEMATIC BACKGROUND:</center></h1> </a>","metadata":{}},{"cell_type":"markdown","source":"## <a id='1.1'> 1.1) Minkowski Distance Metric: </a>\n\nMinkowski Distance is the generalized form of Euclidean and Manhattan Distance.\n\nThe formula for Minkowski Distance is given as:\n\n![](https://cdn-images-1.medium.com/max/1067/1*Fb22fnJRABGUANpjcjwEow.png)\n\nThe p parameter of the Minkowski Distance metric represents the order of the norm. \n\nWhen p is set to 1, the calculation is the same as the Manhattan distance. When p is set to 2, it is the same as the Euclidean distance.\n\np=1: Manhattan distance.\n\np=2: Euclidean distance.\n\np= infinitive, Chebychev Distance\n\nIntermediate values provide a controlled balance between the two measures.\n\nIt is common to use Minkowski distance when implementing a machine learning algorithm that uses distance measures as it gives control over the type of distance measure used for real-valued vectors via a hyperparameter “p” that can be tuned.","metadata":{}},{"cell_type":"markdown","source":"## <a id='1.2'> 1.2) Euclidean Distance Metric: </a>","metadata":{}},{"cell_type":"markdown","source":"Euclidean Distance represents the shortest distance between two points.\n\nMost machine learning algorithms including K-Means use this distance metric to measure the similarity between observations. Let’s say we have two points as shown below:\n\n![](https://cdn-images-1.medium.com/max/1067/1*p1BaA9Px8PimHUuz1V6DMA.png)\n\nSo, the Euclidean Distance between these two points A and B will be:\n\n![](https://cdn-images-1.medium.com/max/1067/1*RwxPrdfS0G0w68yAdw-6Cw.png)\n\nHere’s the formula for Euclidean Distance:\n\n![](https://cdn-images-1.medium.com/max/1067/1*_ftwbnr74RtEwnquHPg2rg.png)\n\nWe can generalize this for an n-dimensional space as:\n\n![](https://cdn-images-1.medium.com/max/1067/1*lHQbNP1Grabz0ViiPM9uow.png)\n\nWhere,\n\nn = number of dimensions\npi, qi = data points","metadata":{}},{"cell_type":"markdown","source":"* ## <a id='1.3'> 1.3) Manhattan Distance Metric: </a>","metadata":{}},{"cell_type":"markdown","source":"Manhattan Distance is the sum of absolute differences between points across all the dimensions.\n\nWe can represent Manhattan Distance as:\n\n![](https://cdn-images-1.medium.com/max/1067/1*kYsOWlz9d7VFWEBYI8Cudg.png)\n\nSince the above representation is 2 dimensional, to calculate Manhattan Distance, we will take the sum of absolute distances in both the x and y directions. So, the Manhattan distance in a 2-dimensional space is given as:\n\n![](https://cdn-images-1.medium.com/max/1067/1*i3kQozqYjq7fj5IHXPlo5g.png)\n\nAnd the generalized formula for an n-dimensional space is given as:\n\n![](https://cdn-images-1.medium.com/max/1067/1*9A3Ni7-UQ4NjDQd4e1kXaA.png)\n\nWhere,\n\nn = number of dimensions\npi, qi = data points","metadata":{}},{"cell_type":"markdown","source":"## <a id='1.4'> 1.4) Cosine Distance & Cosine Similarity Metric: </a>\n\nCosine distance & Cosine Similarity metric is mainly used to find similarities between two data points. As the cosine distance between the data points increases, the cosine similarity, or the amount of similarity decreases, and vice versa.\n\n![](https://miro.medium.com/max/1236/1*23O5Hck8v5flOF9LtvGr8A.png)\n\nIn the above figure, imagine the value of θ to be 60 degrees, then by cosine similarity formula, Cos 60 =0.5 and Cosine distance is 1- 0.5 = 0.5. Therefore the points are 50% similar to each other.","metadata":{}},{"cell_type":"markdown","source":"## <a id='1.5'> 1.5) Jensen-Shannon Distance Metric: </a>\n\nThe Jensen functions is the right metric for calculating distances between probability distribution\n\n![](https://miro.medium.com/max/1400/1*viATYZeg9SiT-ZdzYGjKYA.png)","metadata":{}},{"cell_type":"markdown","source":"## <a id='2'> <h1><center> 2. CODE IMPLEMENTATION:</center></h1> </a>","metadata":{}},{"cell_type":"markdown","source":"#  <h1><center>Dataset Overview: </h1></center>\n\n","metadata":{}},{"cell_type":"code","source":" import pandas as pd # Import library\ndf=pd.read_csv('../input/medical-ratio1/Medical Ratio.csv')\npd.set_option('display.max_columns', None) # display all feature\ndf=df.drop([38])\ndf","metadata":{"execution":{"iopub.status.busy":"2022-04-13T14:11:55.829507Z","iopub.execute_input":"2022-04-13T14:11:55.830286Z","iopub.status.idle":"2022-04-13T14:11:56.401502Z","shell.execute_reply.started":"2022-04-13T14:11:55.830249Z","shell.execute_reply":"2022-04-13T14:11:56.400554Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-13T14:11:56.403553Z","iopub.execute_input":"2022-04-13T14:11:56.403897Z","iopub.status.idle":"2022-04-13T14:11:56.410976Z","shell.execute_reply.started":"2022-04-13T14:11:56.403853Z","shell.execute_reply":"2022-04-13T14:11:56.409831Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"## The dataset contain 38 observations corresponds to 37 Pharmaceutical Companies and the Ratio Rows here represents the Ratio: **hypothetical proportion (average) firm**  called benchmark company\n \n\n## 273 feature corresponds to 273 sub-IPC (International Patent Classification)\n\n## Each sub-IPC represents different technology \n\n## Our goal is to define the list of the companies having the same technology to the Ratio. In other words, we calcuate the distance of the sub-IPC of the companies to the Ratio using different Distance Metric. \n\n## The smaller distance of a company to the Ratio, The higher similarity of that company to the Domain Technology.\n","metadata":{}},{"cell_type":"code","source":"df1=df.copy()\ndf1= df1.drop(columns='COMPANY')\ndf1","metadata":{"execution":{"iopub.status.busy":"2022-04-13T14:11:56.412461Z","iopub.execute_input":"2022-04-13T14:11:56.412769Z","iopub.status.idle":"2022-04-13T14:11:56.955945Z","shell.execute_reply.started":"2022-04-13T14:11:56.412732Z","shell.execute_reply":"2022-04-13T14:11:56.955001Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"## <a id='i=2.1'> 2.1) Minkowski Distance Metric: </a>","metadata":{}},{"cell_type":"code","source":"from scipy.spatial.distance import cdist\nm_dist=pd.DataFrame(cdist(df1,df1,metric='minkowski', p=3))\n#diplay the Minkowski Distance:\nm_dist","metadata":{"execution":{"iopub.status.busy":"2022-04-13T14:11:56.957970Z","iopub.execute_input":"2022-04-13T14:11:56.958221Z","iopub.status.idle":"2022-04-13T14:11:57.047463Z","shell.execute_reply.started":"2022-04-13T14:11:56.958191Z","shell.execute_reply":"2022-04-13T14:11:57.046455Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# Distance of Compaines to Ratio\nnp.warnings.filterwarnings('ignore')\nMinkowski= m_dist.iloc[37:38, :] \nMinkowski.columns=df['COMPANY']\npd.set_option('display.max_columns', None)\nMinkowski.rename(index={37:'Minkowski'},inplace =True)\nMinkowski","metadata":{"execution":{"iopub.status.busy":"2022-04-13T14:11:57.048720Z","iopub.execute_input":"2022-04-13T14:11:57.048968Z","iopub.status.idle":"2022-04-13T14:11:57.084119Z","shell.execute_reply.started":"2022-04-13T14:11:57.048939Z","shell.execute_reply":"2022-04-13T14:11:57.083062Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# Top 10 companies with Highest Technology Similarity to the Domain Technology:\nMinkowski_report = Minkowski.transpose()\nMinkowski_report.reset_index()\nMinkowski_report = Minkowski_report.sort_values(by='Minkowski',ascending= True).head(11)\nMinkowski_report","metadata":{"execution":{"iopub.status.busy":"2022-04-13T14:11:57.085209Z","iopub.execute_input":"2022-04-13T14:11:57.085831Z","iopub.status.idle":"2022-04-13T14:11:57.104717Z","shell.execute_reply.started":"2022-04-13T14:11:57.085793Z","shell.execute_reply":"2022-04-13T14:11:57.104080Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"## <a href='#2.2'> 2.2) Eucclidean Distance Metric: </a>\n","metadata":{}},{"cell_type":"code","source":"from scipy.spatial.distance import cdist\ne_dist=pd.DataFrame(cdist(df1,df1,metric='euclidean'))\n#diplay the Minkowski Distance:\ne_dist","metadata":{"execution":{"iopub.status.busy":"2022-04-13T14:11:57.106049Z","iopub.execute_input":"2022-04-13T14:11:57.106287Z","iopub.status.idle":"2022-04-13T14:11:57.199283Z","shell.execute_reply.started":"2022-04-13T14:11:57.106259Z","shell.execute_reply":"2022-04-13T14:11:57.198651Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# Distance of Compaines to Ratio\nnp.warnings.filterwarnings('ignore')\nEuclidean= e_dist.iloc[37:38, :] \nEuclidean.columns=df['COMPANY']\npd.set_option('display.max_columns', None)\nEuclidean.rename(index={37:'Euclidean'},inplace =True)\nEuclidean","metadata":{"execution":{"iopub.status.busy":"2022-04-13T14:11:57.200461Z","iopub.execute_input":"2022-04-13T14:11:57.200685Z","iopub.status.idle":"2022-04-13T14:11:57.233997Z","shell.execute_reply.started":"2022-04-13T14:11:57.200658Z","shell.execute_reply":"2022-04-13T14:11:57.233295Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# Top 10 companies with Highest Technology Similarity to the Domain Technology:\nEuclidean_report = Euclidean.transpose()\nEuclidean_report.reset_index()\nEuclidean_report= Euclidean_report.sort_values(by='Euclidean',ascending= True).head(11)\nEuclidean_report","metadata":{"execution":{"iopub.status.busy":"2022-04-13T14:11:57.235586Z","iopub.execute_input":"2022-04-13T14:11:57.235831Z","iopub.status.idle":"2022-04-13T14:11:57.248285Z","shell.execute_reply.started":"2022-04-13T14:11:57.235793Z","shell.execute_reply":"2022-04-13T14:11:57.247190Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"## <a id='2.3'> 2.3) Mahattan Distance Metric: </a>","metadata":{}},{"cell_type":"code","source":"from scipy.spatial.distance import cdist\nm_dist=pd.DataFrame(cdist(df1,df1,metric='cityblock'))\n#diplay the Minkowski Distance:\nm_dist","metadata":{"execution":{"iopub.status.busy":"2022-04-13T14:11:57.251592Z","iopub.execute_input":"2022-04-13T14:11:57.251992Z","iopub.status.idle":"2022-04-13T14:11:57.338318Z","shell.execute_reply.started":"2022-04-13T14:11:57.251946Z","shell.execute_reply":"2022-04-13T14:11:57.337527Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# Distance of Compaines to Ratio\nnp.warnings.filterwarnings('ignore')\nMahattan= m_dist.iloc[37:38, :] \nMahattan.columns=df['COMPANY']\npd.set_option('display.max_columns', None)\nMahattan.rename(index={37:'Mahattan'},inplace =True)\nMahattan","metadata":{"execution":{"iopub.status.busy":"2022-04-13T14:11:57.339630Z","iopub.execute_input":"2022-04-13T14:11:57.339940Z","iopub.status.idle":"2022-04-13T14:11:57.376236Z","shell.execute_reply.started":"2022-04-13T14:11:57.339907Z","shell.execute_reply":"2022-04-13T14:11:57.375603Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# Top 10 companies with Highest Technology Similarity to the Domain Technology:\nMahattan_report = Mahattan.transpose()\nMahattan_report.reset_index()\nMahattan_report= Mahattan_report.sort_values(by='Mahattan',ascending= True).head(11)\nMahattan_report","metadata":{"execution":{"iopub.status.busy":"2022-04-13T14:11:57.377343Z","iopub.execute_input":"2022-04-13T14:11:57.378240Z","iopub.status.idle":"2022-04-13T14:11:57.390491Z","shell.execute_reply.started":"2022-04-13T14:11:57.378203Z","shell.execute_reply":"2022-04-13T14:11:57.389894Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"  ## <a id='2.4'> 2.4) Cosine Distance Metric: </a>","metadata":{}},{"cell_type":"code","source":"from scipy.spatial.distance import cdist\nco_dist=pd.DataFrame(cdist(df1,df1,metric='cosine'))\n#diplay the Minkowski Distance:\nco_dist","metadata":{"execution":{"iopub.status.busy":"2022-04-13T14:11:57.391508Z","iopub.execute_input":"2022-04-13T14:11:57.392281Z","iopub.status.idle":"2022-04-13T14:11:57.492629Z","shell.execute_reply.started":"2022-04-13T14:11:57.392243Z","shell.execute_reply":"2022-04-13T14:11:57.492044Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# Distance of Compaines to Ratio\nnp.warnings.filterwarnings('ignore')\nCosine= co_dist.iloc[37:38, :] \nCosine.columns=df['COMPANY']\npd.set_option('display.max_columns', None)\nCosine.rename(index={37:'Cosine'},inplace =True)\nCosine","metadata":{"execution":{"iopub.status.busy":"2022-04-13T14:11:57.493742Z","iopub.execute_input":"2022-04-13T14:11:57.494380Z","iopub.status.idle":"2022-04-13T14:11:57.527928Z","shell.execute_reply.started":"2022-04-13T14:11:57.494336Z","shell.execute_reply":"2022-04-13T14:11:57.527002Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# Top 10 companies with Highest Technology Similarity to the Domain Technology:\nCosine_report = Cosine.transpose()\nCosine_report.reset_index()\nCosine_report =Cosine_report.sort_values(by='Cosine',ascending= True).head(11)\nCosine_report","metadata":{"execution":{"iopub.status.busy":"2022-04-13T14:11:57.529171Z","iopub.execute_input":"2022-04-13T14:11:57.529879Z","iopub.status.idle":"2022-04-13T14:11:57.548810Z","shell.execute_reply.started":"2022-04-13T14:11:57.529844Z","shell.execute_reply":"2022-04-13T14:11:57.548165Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"  ## <a id='2.5'> 2.5) Jensenshannon Distance Metric: </a>","metadata":{}},{"cell_type":"code","source":"from scipy.spatial.distance import cdist\nj_dist=pd.DataFrame(cdist(df1,df1,metric='jensenshannon'))\n#diplay the Minkowski Distance:\nj_dist","metadata":{"execution":{"iopub.status.busy":"2022-04-13T14:11:57.549905Z","iopub.execute_input":"2022-04-13T14:11:57.550268Z","iopub.status.idle":"2022-04-13T14:11:57.645149Z","shell.execute_reply.started":"2022-04-13T14:11:57.550231Z","shell.execute_reply":"2022-04-13T14:11:57.644551Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# Distance of Compaines to Ratio\nnp.warnings.filterwarnings('ignore')\nJensenshannon= j_dist.iloc[37:38, :] \nJensenshannon.columns=df['COMPANY']\npd.set_option('display.max_columns', None)\nJensenshannon.rename(index={37:'Jensenshannon'},inplace =True)\nJensenshannon","metadata":{"execution":{"iopub.status.busy":"2022-04-13T14:11:57.646216Z","iopub.execute_input":"2022-04-13T14:11:57.646584Z","iopub.status.idle":"2022-04-13T14:11:57.679665Z","shell.execute_reply.started":"2022-04-13T14:11:57.646551Z","shell.execute_reply":"2022-04-13T14:11:57.678827Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"# Top 10 companies with Highest Technology Similarity to the Domain Technology:\nJensenshannon_report = Jensenshannon.transpose()\nJensenshannon_report.reset_index()\nJensenshannon_report =Jensenshannon_report.sort_values(by='Jensenshannon',ascending= True).head(11)\nJensenshannon_report","metadata":{"execution":{"iopub.status.busy":"2022-04-13T14:11:57.680956Z","iopub.execute_input":"2022-04-13T14:11:57.681798Z","iopub.status.idle":"2022-04-13T14:11:57.694841Z","shell.execute_reply.started":"2022-04-13T14:11:57.681751Z","shell.execute_reply":"2022-04-13T14:11:57.694035Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"from IPython.display import display, HTML\n\ncss = \"\"\"\n.output {\n    flex-direction: row;\n}\n\"\"\"\n\nHTML('<style>{}</style>'.format(css))","metadata":{"execution":{"iopub.status.busy":"2022-04-13T14:11:57.696187Z","iopub.execute_input":"2022-04-13T14:11:57.697005Z","iopub.status.idle":"2022-04-13T14:11:57.708752Z","shell.execute_reply.started":"2022-04-13T14:11:57.696962Z","shell.execute_reply":"2022-04-13T14:11:57.707848Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"  ## <a id='2.6'> 2.6) : </a> DISTANCE METRIC RESULT SUMMAY:","metadata":{}},{"cell_type":"code","source":"from IPython.display import display_html \n\n\ndf1_style = Minkowski_report.style.set_table_attributes(\"style='display:inline; margin-right:20px;'\").set_caption(\"DF One\")\ndf2_style = Euclidean_report.style.set_table_attributes(\"style='display:inline'\").set_caption(\"DF Two\")\ndf3_style = Mahattan_report.style.set_table_attributes(\"style='display:inline'\").set_caption(\"DF Three\")\ndf4_style = Cosine_report.style.set_table_attributes(\"style='display:inline'\").set_caption(\"DF Four\")\ndf5_style = Jensenshannon_report.style.set_table_attributes(\"style='display:inline'\").set_caption(\"DF Five\")\ndisplay_html(df1_style._repr_html_() + df2_style._repr_html_() + df3_style._repr_html_()+ df4_style._repr_html_()+df5_style._repr_html_(), raw=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-13T14:11:57.711980Z","iopub.execute_input":"2022-04-13T14:11:57.712215Z","iopub.status.idle":"2022-04-13T14:11:57.733147Z","shell.execute_reply.started":"2022-04-13T14:11:57.712187Z","shell.execute_reply":"2022-04-13T14:11:57.731961Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"## <a id='3'> <h1><center> 3. COMPANY GROUPING:</center></h1> </a>","metadata":{}},{"cell_type":"markdown","source":"  ## <a id='3.1'> 3.1) : </a> K-MEAN CLUSTER: ELBOW METHOD:","metadata":{}},{"cell_type":"markdown","source":"It is the simplest and commonly used iterative type unsupervised learning algorithm. In this, we randomly initialize the K number of centroids in the data (the number of k is found using the Elbow method which will be discussed later in this article ) and iterates these centroids until no change happens to the position of the centroid. Let’s go through the steps involved in K means clustering for a better understanding.\n\n1) Select the number of clusters for the dataset ( K )\n\n2) Select K number of centroids\n\n3) By calculating the Euclidean distance or Manhattan distance assign the points to the nearest centroid, thus creating K groups\n\n4) Now find the original centroid in each group\n\n5) Again reassign the whole data point based on this new centroid, then repeat step 4 until the position of the centroid doesn’t change.\n\nFinding the optimal number of clusters is an important part of this algorithm. A commonly used method for finding optimal K value is Elbow Method.\n\nIn the Elbow method, we are actually varying the number of clusters ( K ) from 1 – 10. For each value of K, we are calculating WCSS ( Within-Cluster Sum of Square ).\n\nAs the number of clusters increases, the WCSS value will start to decrease. WCSS value is largest when K = 1. When we analyze the graph we can see that the graph will rapidly change at a point and thus creating an elbow shape.\n\nThe K value corresponding to this point is the optimal K value or an optimal number of clusters\n\nIN THIS KERNEL I WILL APPLY RESULT FROM COSINE DISTANCE TO CLUSTTERING","metadata":{}},{"cell_type":"code","source":"\n# Import Library:\nfrom yellowbrick.cluster import KElbowVisualizer\nfrom sklearn.cluster import KMeans\n# Elbow Method for K means\nmodel = KMeans()\n# k is range of number of clusters.\nvisualizer = KElbowVisualizer(model, k=(1,11), timings= False)\nvisualizer.fit(co_dist)        # Fit data to visualizer\nvisualizer.show()        # Finalize and render figure","metadata":{"execution":{"iopub.status.busy":"2022-04-13T14:11:57.734881Z","iopub.execute_input":"2022-04-13T14:11:57.735190Z","iopub.status.idle":"2022-04-13T14:11:59.044635Z","shell.execute_reply.started":"2022-04-13T14:11:57.735147Z","shell.execute_reply":"2022-04-13T14:11:59.043880Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n#Transform the data\npca = PCA()\ntransform = pca.fit_transform(co_dist)\n#Initialize the class object\nkmeans = KMeans(n_clusters= 2)\n \n#predict the labels of clusters.\nlabel = kmeans.fit_predict(transform)\n \ndata={'Name':df['COMPANY'],'Cluster':label}\nelbow_result=pd.DataFrame(data)\nelbow_result","metadata":{"execution":{"iopub.status.busy":"2022-04-13T14:12:28.292259Z","iopub.execute_input":"2022-04-13T14:12:28.293152Z","iopub.status.idle":"2022-04-13T14:12:28.352631Z","shell.execute_reply.started":"2022-04-13T14:12:28.293097Z","shell.execute_reply":"2022-04-13T14:12:28.351622Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"  ## <a id='3.2'> 3.2)  </a> K-MEAN CLUSTER: SIHOUETTE METHOD:","metadata":{}},{"cell_type":"code","source":"model = KMeans()\n# k is range of number of clusters.\nvisualizer = KElbowVisualizer(model, k=(2,11),metric='silhouette', timings= False)\nvisualizer.fit(co_dist)        # Fit the data to the visualizer\nvisualizer.show()        # Finalize and render the figure","metadata":{"execution":{"iopub.status.busy":"2022-04-13T14:12:30.871162Z","iopub.execute_input":"2022-04-13T14:12:30.872094Z","iopub.status.idle":"2022-04-13T14:12:32.382857Z","shell.execute_reply.started":"2022-04-13T14:12:30.872050Z","shell.execute_reply":"2022-04-13T14:12:32.381976Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"  ## <a id='3.3'> 3.3) </a> AGGLOMERATIVE HIERARCHICAL CLUSTERING ALGORITHM:","metadata":{}},{"cell_type":"markdown","source":"In agglomerative clustering, the cluster formation starts with individual points. Each point is considered as one cluster.\n\nLet’s say there are N data points. In the beginning, there will be N clusters.\n\nThen, the distance between each pair of cluster is found and the clusters closest to each other is matched and made as one cluster. This would result in (N – 1) cluster.\n\nIn the next step, the distance between pair of clusters are found and the clusters closest to each other is matched and made as one cluster.\n\nThis would result in (N – 2) clusters. The same process is repeated until all the data points are merged into one cluster. e.g., root cluster.\n\n","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport scipy.cluster.hierarchy as shc\nplt.figure(figsize=(10, 7))  \nplt.title(\"Dendrograms\")  \ndend = shc.dendrogram(shc.linkage(co_dist, method='ward'))\n","metadata":{"execution":{"iopub.status.busy":"2022-04-13T14:13:05.610782Z","iopub.execute_input":"2022-04-13T14:13:05.611193Z","iopub.status.idle":"2022-04-13T14:13:06.184373Z","shell.execute_reply.started":"2022-04-13T14:13:05.611148Z","shell.execute_reply":"2022-04-13T14:13:06.183398Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 7))  \nplt.title(\"Dendrograms\")  \ndend = shc.dendrogram(shc.linkage(co_dist, method='ward'))\nplt.axhline(y=2.5, color='r', linestyle='--')","metadata":{"execution":{"iopub.status.busy":"2022-04-13T14:13:09.395199Z","iopub.execute_input":"2022-04-13T14:13:09.395489Z","iopub.status.idle":"2022-04-13T14:13:10.140738Z","shell.execute_reply.started":"2022-04-13T14:13:09.395445Z","shell.execute_reply":"2022-04-13T14:13:10.139769Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"## * k= 3","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering\ncluster = AgglomerativeClustering(n_clusters= 3 , affinity='euclidean', linkage='ward')  \nlabel= cluster.fit_predict(co_dist)\n\ndata={'Name':df['COMPANY'],'Cluster':label}\nAgg_result3=pd.DataFrame(data)\nAgg_result3","metadata":{"execution":{"iopub.status.busy":"2022-04-13T14:13:12.493816Z","iopub.execute_input":"2022-04-13T14:13:12.494104Z","iopub.status.idle":"2022-04-13T14:13:12.511210Z","shell.execute_reply.started":"2022-04-13T14:13:12.494074Z","shell.execute_reply":"2022-04-13T14:13:12.510550Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 7))  \nplt.title(\"Dendrograms\")  \ndend = shc.dendrogram(shc.linkage(co_dist, method='ward'))\nplt.axhline(y=2, color='r', linestyle='--')","metadata":{"execution":{"iopub.status.busy":"2022-04-13T14:13:14.673332Z","iopub.execute_input":"2022-04-13T14:13:14.673813Z","iopub.status.idle":"2022-04-13T14:13:15.277456Z","shell.execute_reply.started":"2022-04-13T14:13:14.673759Z","shell.execute_reply":"2022-04-13T14:13:15.276619Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"## * k=5","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering\ncluster = AgglomerativeClustering(n_clusters= 5 , affinity='euclidean', linkage='ward')  \nlabel= cluster.fit_predict(co_dist)\n\ndata={'Name':df['COMPANY'],'Cluster':label}\nAgg_result5=pd.DataFrame(data)\nAgg_result5","metadata":{"execution":{"iopub.status.busy":"2022-04-13T14:13:17.663345Z","iopub.execute_input":"2022-04-13T14:13:17.663908Z","iopub.status.idle":"2022-04-13T14:13:17.680819Z","shell.execute_reply.started":"2022-04-13T14:13:17.663868Z","shell.execute_reply":"2022-04-13T14:13:17.680202Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":"  ## <a id='3.4'> 3.4) </a> COMPANY GROUPING SUMMAY:","metadata":{}},{"cell_type":"code","source":"df1_style = elbow_result.style.set_table_attributes(\"style='display:inline; margin-right:20px;'\").set_caption(\"Ebow_result\")\ndf2_style = Agg_result5.style.set_table_attributes(\"style='display:inline'\").set_caption(\"Agg_result5\")\ndf3_style = Agg_result3.style.set_table_attributes(\"style='display:inline'\").set_caption(\"Agg_result3\")\ndisplay_html(df1_style._repr_html_() + df2_style._repr_html_() + df3_style._repr_html_(), raw=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-13T14:13:20.510876Z","iopub.execute_input":"2022-04-13T14:13:20.511206Z","iopub.status.idle":"2022-04-13T14:13:20.534577Z","shell.execute_reply.started":"2022-04-13T14:13:20.511174Z","shell.execute_reply":"2022-04-13T14:13:20.533267Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}